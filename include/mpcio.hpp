#ifndef __MCPIO_HPP__
#define __MCPIO_HPP__

#include <array>
#include <atomic>
#include <bsd/stdlib.h> // arc4random_buf
#include <deque>
#include <fstream>
#include <iostream>
#include <optional>
#include <queue>
#include <string>
#include <vector>

#include <boost/asio.hpp>
#include <boost/chrono.hpp>
#include <boost/thread.hpp>

#include "corotypes.hpp"
#include "types.hpp"

using boost::asio::ip::tcp;

// Classes to represent stored precomputed data (e.g., multiplication triples)

template <typename T, typename N> class PreCompStorage {
public:
  PreCompStorage() : name(N::name), depth(0), count(0) {}
  PreCompStorage(unsigned player, ProcessingMode mode,
                 const char *filenameprefix, unsigned thread_num);
  void init(unsigned player, ProcessingMode mode, const char *filenameprefix,
            unsigned thread_num, nbits_t depth = 0, nbits_t width = 1);
  void get(T &nextval);

  inline void inc() { ++count; }
  inline size_t get_stats() { return count; }
  inline void reset_stats() { count = 0; }

private:
  std::ifstream storage;
  std::string name;
  nbits_t depth;
  nbits_t width;
  size_t count;
};

// If we want to send Lamport clocks in messages, define this.  It adds
// an 8-byte header to each message (length and Lamport clock), so it
// has a small network cost.  We always define and pass the Lamport
// clock member of MPCIO to the IO functions for simplicity, but they're
// ignored if this isn't defined
#define SEND_LAMPORT_CLOCKS
using lamport_t = uint32_t;
using atomic_lamport_t = std::atomic<lamport_t>;
using opt_lamport_t = std::optional<lamport_t>;
#ifdef SEND_LAMPORT_CLOCKS
struct MessageWithHeader {
  std::string header;
  std::string message;

  MessageWithHeader(std::string &&msg, lamport_t lamport)
      : message(std::move(msg)) {
    char hdr[sizeof(uint32_t) + sizeof(lamport_t)];
    uint32_t msglen = uint32_t(message.size());
    memmove(hdr, &msglen, sizeof(msglen));
    memmove(hdr + sizeof(msglen), &lamport, sizeof(lamport));
    header.assign(hdr, sizeof(hdr));
  }
};
#endif

// A class to wrap a socket to another MPC party.  This wrapping allows
// us to do some useful logging, and perform async_writes transparently
// to the application.

class MPCSingleIO {
  tcp::socket sock;
  size_t totread, totwritten;
  std::string dest;
  int thread_num;
#ifdef RECORD_IOTRACE
  std::vector<ssize_t> iotrace;
#endif

  // To avoid blocking if both we and our peer are trying to send
  // something very large, and neither side is receiving, we will send
  // with async_write.  But this has a number of implications:
  // - The data to be sent has to be copied into this MPCSingleIO,
  //   since asio::buffer pointers are not guaranteed to remain valid
  //   after the end of the coroutine that created them
  // - We have to keep a queue of messages to be sent, in case
  //   coroutines call send() before the previous message has finished
  //   being sent
  // - This queue may be accessed from the async_write thread as well
  //   as the work thread that uses this MPCSingleIO directly (there
  //   should be only one of the latter), so we need some locking

  // This is where we accumulate data passed in queue()
  std::string dataqueue;

  // When send() is called, the above dataqueue is appended to this
  // messagequeue, and the dataqueue is reset.  If messagequeue was
  // empty before this append, launch async_write to write the first
  // thing in the messagequeue.  When async_write completes, it will
  // delete the first thing in the messagequeue, and see if there are
  // any more elements.  If so, it will start another async_write.
  // The invariant is that there is an async_write currently running
  // iff messagequeue is nonempty.
#ifdef SEND_LAMPORT_CLOCKS
  std::queue<MessageWithHeader> messagequeue;
#else
  std::queue<std::string> messagequeue;
#endif

  // If a single message is broken into chunks in order to get the
  // first part of it out on the wire while the rest of it is still
  // being computed, we want the Lamport clock of all the chunks to be
  // that of when the message is first created.  This value will be
  // nullopt when there has been no queue() since the last explicit
  // send() (as opposed to the implicit send() called by queue()
  // itself if it wants to get a chunk on its way), and will be set to
  // the current lamport clock when that first queue() after each
  // explicit send() happens.
  opt_lamport_t message_lamport;

#ifdef SEND_LAMPORT_CLOCKS
  // If Lamport clocks are being sent, then the data stream is divided
  // into chunks, each with a header containing the length of the
  // chunk and the Lamport clock.  So when we read, we'll read a whole
  // chunk, and store it here.  Then calls to recv() will read pieces
  // of this buffer until it has all been read, and then read the next
  // header and chunk.
  std::string recvdata;
  size_t recvdataremain;
#endif

  // Never touch the above messagequeue without holding this lock (you
  // _can_ touch the strings it contains, though, if you looked one up
  // while holding the lock).
  boost::mutex messagequeuelock;

  // Asynchronously send the first message from the message queue.
  // * The messagequeuelock must be held when this is called! *
  // This method may be called from either thread (the work thread or
  // the async_write handler thread).
  void async_send_from_msgqueue();

public:
  MPCSingleIO(tcp::socket &&sock, const char *dest, int thread_num)
      : sock(std::move(sock)), totread(0), totwritten(0), dest(dest),
        thread_num(thread_num)
#ifdef SEND_LAMPORT_CLOCKS
        ,
        recvdataremain(0)
#endif
  {
  }

  // Returns 1 if a new message is started, 0 otherwise
  size_t queue(const void *data, size_t len, lamport_t lamport);

  void send(bool implicit_send = false);

  size_t recv(void *data, size_t len, lamport_t &lamport);

#ifdef RECORD_IOTRACE
  void dumptrace(std::ostream &os, const char *label = NULL);

  void resettrace() { iotrace.clear(); }
#endif
};

// A base class to represent all of a computation peer or server's IO,
// either to other parties or to local storage (the computation and
// server cases are separate subclasses below).

struct MPCIO {
  int player;
  ProcessingMode mode;
  size_t num_threads;
  atomic_lamport_t lamport;
  std::vector<size_t> msgs_sent;
  std::vector<size_t> msg_bytes_sent;
  std::vector<size_t> aes_ops;
  boost::chrono::steady_clock::time_point steady_start;
  boost::chrono::process_cpu_clock::time_point cpu_start;

  MPCIO(int player, ProcessingMode mode, size_t num_threads)
      : player(player), mode(mode), num_threads(num_threads), lamport(0) {
    reset_stats();
  }

  void reset_stats();

  static void dump_memusage(std::ostream &os);

  void dump_stats(std::ostream &os);

  // Make MPCIO objects non-copyable; otherwise the Lamport clock
  // gets duplicated and not kept in sync.
  MPCIO(const MPCIO &) = delete;
  MPCIO &operator=(const MPCIO &) = delete;
};

// A class to represent all of a computation peer's IO, either to other
// parties or to local storage

struct MPCPeerIO : public MPCIO {
  // We use a deque here instead of a vector because you can't have a
  // vector of a type without a copy constructor (tcp::socket is the
  // culprit), but you can have a deque of those for some reason.
  std::deque<MPCSingleIO> peerios;
  std::deque<MPCSingleIO> serverios;
  std::vector<PreCompStorage<MultTriple, MultTripleName>> multtriples;
  std::vector<PreCompStorage<HalfTriple, HalfTripleName>> halftriples;
  std::vector<PreCompStorage<AndTriple, AndTripleName>> andtriples;
  std::vector<PreCompStorage<SelectTriple<value_t>, ValSelectTripleName>>
      valselecttriples;
  std::vector<PreCompStorage<CDPF, CDPFName>> cdpfs;
  // The outer vector is (like above) one item per thread
  // The inner array is indexed by DPF depth (depth d is at entry d-1)
  // We have one of these whole vectors-of-arrays for each RDPF width,
  // wrapped into a tuple
  template <nbits_t WIDTH>
  using RDPFPrecomps =
      std::vector<std::array<PreCompStorage<RDPFTriple<WIDTH>, RDPFTripleName>,
                             ADDRESS_MAX_BITS>>;
  template <nbits_t WIDTH>
  using IRDPFPrecomps =
      std::vector<std::array<PreCompStorage<RDPFTriple<WIDTH>, IRDPFTripleName>,
                             ADDRESS_MAX_BITS>>;
  std::tuple<RDPFPrecomps<1>, RDPFPrecomps<2>, RDPFPrecomps<3>, RDPFPrecomps<4>,
             RDPFPrecomps<5>>
      rdpftriples;
  std::tuple<IRDPFPrecomps<1>, IRDPFPrecomps<2>, IRDPFPrecomps<3>,
             IRDPFPrecomps<4>, IRDPFPrecomps<5>>
      irdpftriples;

  MPCPeerIO(unsigned player, ProcessingMode mode,
            std::deque<tcp::socket> &peersocks,
            std::deque<tcp::socket> &serversocks);

  void dump_precomp_stats(std::ostream &os);

  void reset_precomp_stats();

  void dump_stats(std::ostream &os);
};

// A class to represent all of the server party's IO, either to
// computational parties or to local storage

struct MPCServerIO : public MPCIO {
  std::deque<MPCSingleIO> p0ios;
  std::deque<MPCSingleIO> p1ios;
  // The outer vector is (like above) one item per thread
  // The inner array is indexed by DPF depth (depth d is at entry d-1)
  // We have one of these whole vectors-of-arrays for each RDPF width,
  // wrapped into a tuple
  template <nbits_t WIDTH>
  using RDPFPrecomps =
      std::vector<std::array<PreCompStorage<RDPFPair<WIDTH>, RDPFPairName>,
                             ADDRESS_MAX_BITS>>;
  template <nbits_t WIDTH>
  using IRDPFPrecomps =
      std::vector<std::array<PreCompStorage<RDPFPair<WIDTH>, IRDPFPairName>,
                             ADDRESS_MAX_BITS>>;
  std::tuple<RDPFPrecomps<1>, RDPFPrecomps<2>, RDPFPrecomps<3>, RDPFPrecomps<4>,
             RDPFPrecomps<5>>
      rdpfpairs;
  std::tuple<IRDPFPrecomps<1>, IRDPFPrecomps<2>, IRDPFPrecomps<3>,
             IRDPFPrecomps<4>, IRDPFPrecomps<5>>
      irdpfpairs;

  MPCServerIO(ProcessingMode mode, std::deque<tcp::socket> &p0socks,
              std::deque<tcp::socket> &p1socks);

  void dump_precomp_stats(std::ostream &os);

  void reset_precomp_stats();

  void dump_stats(std::ostream &os);
};

class MPCSingleIOStream {
  MPCSingleIO &sio;
  lamport_t &lamport;
  size_t &msgs_sent;
  size_t &msg_bytes_sent;

public:
  MPCSingleIOStream(MPCSingleIO &sio, lamport_t &lamport, size_t &msgs_sent,
                    size_t &msg_bytes_sent)
      : sio(sio), lamport(lamport), msgs_sent(msgs_sent),
        msg_bytes_sent(msg_bytes_sent) {}

  MPCSingleIOStream &write(const char *data, std::streamsize len) {
    size_t newmsg = sio.queue(data, len, lamport);
    msgs_sent += newmsg;
    msg_bytes_sent += len;
    return *this;
  }

  MPCSingleIOStream &read(char *data, std::streamsize len) {
    sio.recv(data, len, lamport);
    return *this;
  }
};

// A handle to one thread's sockets and streams in a MPCIO

class MPCTIO {
  int thread_num;

  // The number of threads a coroutine using this MPCTIO can use for
  // local computation (no communication and no yielding).  Multiple
  // coroutines with the same MPCTIO can have this value larger than
  // 1, since they will not be able to use multiple threads at the
  // same time.
  int local_cpu_nthreads;

  // The number of threads a coroutine using this MPCTIO can launch
  // into separate MPCTIOs with their own communication.  It is
  // important that at most one coroutine using this MPCTIO can have
  // this value set larger than 1, since all MPCTIOs with the same
  // thread_num (and so using the same sockets) have to be controlled
  // by the same run_coroutines(tio, ...) call.
  int communication_nthreads;

  lamport_t thread_lamport;
  MPCIO &mpcio;
  std::optional<MPCSingleIOStream> peer_iostream;
  std::optional<MPCSingleIOStream> server_iostream;
  std::optional<MPCSingleIOStream> p0_iostream;
  std::optional<MPCSingleIOStream> p1_iostream;
#ifdef VERBOSE_COMMS
  size_t round_num;
#endif

  // We implement SelectTriple<bit_t> by fetching a single AndTriple
  // and using it for producing 64 bitwise SelectTriple<bit_t>s.
  AndTriple last_andtriple;
  nbits_t last_andtriple_bits_remaining;

  // We allow for prefetching of SelectTriple<DPFnode>s to save one
  // network round per level when constructing RDPFs
  std::deque<SelectTriple<DPFnode>> queued_nodeselecttriples;
  // For P0 and P1, it should always be the case that
  // remaining_nodesselecttriples equals
  // queued_nodeselecttriples.size().  P2 does not store anything in
  // queued_nodeselecttriples, however.
  size_t remaining_nodesselecttriples;

public:
  // Make MPCTIO objects non-copyable; otherwise the Lamport clock
  // gets duplicated and not kept in sync.
  MPCTIO(const MPCTIO &) = delete;
  MPCTIO &operator=(const MPCTIO &) = delete;

  MPCTIO(MPCIO &mpcio, int thread_num, int num_threads = 1);

  // Sync our per-thread lamport clock with the master one in the
  // mpcio.  You only need to call this explicitly if your MPCTIO
  // outlives your thread (in which case call it after the join), or
  // if your threads do interthread communication amongst themselves
  // (in which case call it in the sending thread before the send, and
  // call it in the receiving thread after the receive).  If you want
  // to call MPCIO::dump_stats() in the middle of a run (while the
  // MPCTIO is still alive), call this as well.
  void sync_lamport();

  // Only call this if you can be sure that there are no outstanding
  // messages in flight, you can call it on all existing MPCTIOs, and
  // you really want to reset the Lamport clock in the midding of a
  // run.
  void reset_lamport();

  // Read the thread_lamport counter, for performance debugging
  lamport_t get_thread_lamport() { return thread_lamport; }

  // The normal case, where the MPCIO is created inside the thread,
  // and so destructed when the thread ends, is handled automatically
  // here.
  ~MPCTIO() {
    send();
    sync_lamport();
  }

  // Computational peers use these functions:

  // Queue up data to the peer or to the server

  void queue_peer(const void *data, size_t len);
  void queue_server(const void *data, size_t len);

  // Receive data from the peer or to the server

  size_t recv_peer(void *data, size_t len);
  size_t recv_server(void *data, size_t len);

  // Or get these MPCSingleIOStreams
  MPCSingleIOStream &iostream_peer() { return peer_iostream.value(); }
  MPCSingleIOStream &iostream_server() { return server_iostream.value(); }

  // The server uses these functions:

  // Queue up data to p0 or p1

  void queue_p0(const void *data, size_t len);
  void queue_p1(const void *data, size_t len);

  // Receive data from p0 or p1

  size_t recv_p0(void *data, size_t len);
  size_t recv_p1(void *data, size_t len);

  // Or get these MPCSingleIOStreams
  MPCSingleIOStream &iostream_p0() { return p0_iostream.value(); }
  MPCSingleIOStream &iostream_p1() { return p1_iostream.value(); }

  // Everyone can use the remaining functions.

  // Send all queued data for this thread

  void send();

  // Functions to get precomputed values.  If we're in the online
  // phase, get them from PreCompStorage.  If we're in the
  // preprocessing phase, read them from the server.

  MultTriple multtriple(yield_t &yield);
  HalfTriple halftriple(yield_t &yield, bool tally = true);
  AndTriple andtriple(yield_t &yield);
  void request_nodeselecttriples(yield_t &yield, size_t num);
  SelectTriple<DPFnode> nodeselecttriple(yield_t &yield);
  SelectTriple<value_t> valselecttriple(yield_t &yield);
  SelectTriple<bit_t> bitselecttriple(yield_t &yield);

  // These ones only work during the online phase
  // Computational peers call:
  template <nbits_t WIDTH = 1>
  RDPFTriple<WIDTH> rdpftriple(yield_t &yield, nbits_t depth,
                               bool incremental = false,
                               bool keep_expansion = true);
  // The server calls:
  template <nbits_t WIDTH = 1>
  RDPFPair<WIDTH> rdpfpair(yield_t &yield, nbits_t depth,
                           bool incremental = false);
  // Anyone can call:
  CDPF cdpf(yield_t &yield);

  // Accessors

  inline int player() { return mpcio.player; }
  inline bool preprocessing() { return mpcio.mode == MODE_PREPROCESSING; }
  inline bool is_server() { return mpcio.player == 2; }
  inline size_t &aes_ops() { return mpcio.aes_ops[thread_num]; }
  inline size_t msgs_sent() { return mpcio.msgs_sent[thread_num]; }
  inline int cpu_nthreads(int nthreads = 0) {
    int res = local_cpu_nthreads;
    if (nthreads > 0) {
      local_cpu_nthreads = nthreads;
    }
    return res;
  }
  inline int comm_nthreads(int nthreads = 0) {
    int res = communication_nthreads;
    if (nthreads > 0) {
      communication_nthreads = nthreads;
    }
    return res;
  }
};

// Set up the socket connections between the two computational parties
// (P0 and P1) and the server party (P2).  For each connection, the
// lower-numbered party does the accept() and the higher-numbered party
// does the connect().

// Computational parties call this version with player=0 or 1

void mpcio_setup_computational(unsigned player,
                               boost::asio::io_context &io_context,
                               const char *p0addr, // can be NULL when player=0
                               int num_threads,
                               std::deque<tcp::socket> &peersocks,
                               std::deque<tcp::socket> &serversocks);

// Server calls this version

void mpcio_setup_server(boost::asio::io_context &io_context, const char *p0addr,
                        const char *p1addr, int num_threads,
                        std::deque<tcp::socket> &p0socks,
                        std::deque<tcp::socket> &p1socks);

#include "mpcio.tcc"

#endif
